# Improving Test Readability using LLMs (DeepSeek, Gemini, LLaMA)

This repository demonstrates a technical approach to enhancing the readability of automatically generated Python test suites using large language models (LLMs). Three LLMs — DeepSeek, Gemini, and LLaMA — were applied to tests originally generated by Pynguin. The objective is to refactor these tests for better structure, clarity, naming, assertions, and maintainability.

---

## 📁 Repository Structure

```
.
├── original_tests/
│   ├── test_queue_example.py
│   ├── test_string_utils_validation.py
│   └── test_httpie_sessions.py
│
├── deepseek_results/
│   ├── test_queue_example_deepseek.py
│   ├── test_string_utils_validation_deepseek.py
│   └── test_httpie_sessions_deepseek.py
│
├── gemini_results/
│   ├── test_queue_example_gemini.py
│   ├── test_string_utils_validation_gemini.py
│   └── test_httpie_sessions_gemini.py
│
├── llama_results/
│   ├── test_queue_example_llama.py
│   ├── test_string_utils_validation_llama.py
│   └── test_httpie_sessions_llama.py
│
├── deepseek.py           # DeepSeek API runner
├── gemini.py             # Gemini API runner
├── meta.py               # LLaMA (Meta) API runner
├── prompt_template.txt   # Prompt used to instruct models
└── README.md
```

---

## ⚙️ Installation & Setup

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/test-readability-enhancer.git
cd test-readability-enhancer
```

### 2. Install Dependencies

```bash
pip install openai
```

### 3. Set OpenRouter API Key

Create a `.env` file or set the key directly in your environment:

```bash
export OPENROUTER_API_KEY="your-api-key"
```

---

## 🚀 How to Run the Demonstration

Each script will take an original test file, prepend a prompt from `prompt_template.txt`, send it to a chosen LLM via OpenRouter, and save the output:

```bash
python deepseek.py   # Uses DeepSeek
python gemini.py     # Uses Gemini
python meta.py       # Uses Meta LLaMA
```

Output files will be saved in `*_results/` directories respectively.

---

## 📊 Readability Comparison Overview

### ✔️ Evaluation Criteria
- Test naming clarity
- Assertion clarity
- Test structure (setup, execution, assertion separation)
- Comment quality and coverage explanation
- Readability and formatting

### 🧪 Files Compared
| Module               | Original       | DeepSeek                | Gemini                  | LLaMA                   |
|----------------------|----------------|--------------------------|--------------------------|--------------------------|
| `queue_example`      | `original_tests/test_queue_example.py` | ✅ Improved structure, better names | ✅ Focused assertions | ✅ Best naming & clarity |
| `string_utils_validation` | `original_tests/test_string_utils_validation.py` | ✅ +Edge cases, +Comments | ✅ Refined assertions | ✅ Very readable layout |
| `httpie_sessions`    | `original_tests/test_httpie_sessions.py` | ✅ Added constants, more readable | ✅ Simplified control flow | ✅ Added setup clarity |

### 🏆 Overall Ranking (Readability)
1. **LLaMA (Meta)** – Best overall readability, naming, and test flow.
2. **DeepSeek** – Slightly verbose but detailed and clean.
3. **Gemini** – Concise and focused, fewer comments.

---

## ✅ Conclusion

The automated approach using concise high-level prompts effectively improved the readability of Pynguin-generated tests. LLaMA produced the clearest structure and variable names, DeepSeek offered rich details and expanded coverage, and Gemini balanced clarity with conciseness.

This repository demonstrates how test suite quality can be enhanced using modern LLMs with minimal manual intervention.

---

## 📬 Feedback

Feel free to fork the repo, open an issue, or contribute improvements!